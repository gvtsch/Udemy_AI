{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Tuple\n",
    "import gym\n",
    "import numpy as np\n",
    "from environment import Env, GraphicDisplay, GOAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: Env) -> None:\n",
    "        self.env = env\n",
    "        self.rows, self.cols = self.env.width, self.env.height\n",
    "        self.S = self.env.all_state\n",
    "        self.A = self.env.possible_actions\n",
    "        self.num_actions = len(self.A)\n",
    "        self.num_states = len(self.S)\n",
    "        self.gamma = 0.9\n",
    "        # Gleichverteilte Policy:\n",
    "        self.init_prob = 1.0 / self.num_actions \n",
    "        self.policy = np.full(shape=(self.rows, self.cols, self.num_actions), fill_value=self.init_prob)\n",
    "        self.v_values = np.zeros(shape=(self.rows, self.cols))\n",
    "\n",
    "    def get_value(self, state: Tuple[int, int]) -> Any:\n",
    "        return self.v_values[state[0]][state[1]]\n",
    "\n",
    "    def get_action(self, state: Tuple[int, int]) -> Any:\n",
    "        if state == GOAL:\n",
    "            return\n",
    "        policy_in_state = self.policy[state[0]][state[1]]\n",
    "        action = np.random.choice(self.A, p=policy_in_state)\n",
    "        return action\n",
    "\n",
    "    def policy_evaluation(self) -> None:\n",
    "        next_v_values = np.zeros(shape=(self.rows, self.cols))\n",
    "\n",
    "        for state in self.S:\n",
    "            value = 0.0\n",
    "            if state == GOAL:\n",
    "                continue\n",
    "            2\n",
    "\n",
    "            for action in self.A:\n",
    "                policy_in_state = self.policy[state[0]][state[1]]\n",
    "                next_state, reward = self.env.step(state, action)\n",
    "                next_v_value = self.v_values[next_state[0]][next_state[1]]\n",
    "                value += policy_in_state[action] * (reward + self.gamma * next_v_value)\n",
    "                \n",
    "            next_v_values[state[0]][state[1]] = value\n",
    "        \n",
    "        self.v_values = next_v_value\n",
    "\n",
    "    def policy_improvement(self) -> None:\n",
    "        next_policy = self.policy\n",
    "\n",
    "        for state in self.S:\n",
    "            if state == GOAL:\n",
    "                continue\n",
    "            \n",
    "            temp_vals = np.zeros(shape=(self.num_actions))\n",
    "            policy_update = np.zeros(shape=(self.num_actions))\n",
    "\n",
    "            for idx, action in enumerate(self.A):\n",
    "                next_state, reward = self.env.step(state, action)\n",
    "                next_v_value = self.v_values[next_state[0]][next_state[1]]\n",
    "                temp_vals[idx], reward + self.gamma * next_v_value\n",
    "\n",
    "            max_indices = np.argwhere(temp_vals == np.max(temp_vals)).ravel()\n",
    "                # argwhere \n",
    "                # ravel\n",
    "            prob = 1.0 / len(max_indices)\n",
    "            for index in max_indices:\n",
    "                policy_update[index] = prob\n",
    "            next_policy[state[0]][state[1]] = policy_update\n",
    "            self.policy = next_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    env = Env()\n",
    "    agent = Agent(env)\n",
    "    grid_world = GraphicDisplay(agent)\n",
    "    grid_world.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Selbststudium\\\\Udemy\\\\Udemy_AI_\\\\Chapter7_2_PolicyIteration/img/up.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter7_2_PolicyIteration\\gridWorldPolicyIterationAgent.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/gridWorldPolicyIterationAgent.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/gridWorldPolicyIterationAgent.ipynb#ch0000003?line=1'>2</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter7_2_PolicyIteration\\gridWorldPolicyIterationAgent.ipynb Cell 3'\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/gridWorldPolicyIterationAgent.ipynb#ch0000002?line=1'>2</a>\u001b[0m env \u001b[39m=\u001b[39m Env()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/gridWorldPolicyIterationAgent.ipynb#ch0000002?line=2'>3</a>\u001b[0m agent \u001b[39m=\u001b[39m Agent(env)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/gridWorldPolicyIterationAgent.ipynb#ch0000002?line=3'>4</a>\u001b[0m grid_world \u001b[39m=\u001b[39m GraphicDisplay(agent)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/gridWorldPolicyIterationAgent.ipynb#ch0000002?line=4'>5</a>\u001b[0m grid_world\u001b[39m.\u001b[39mmainloop()\n",
      "File \u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter7_2_PolicyIteration\\environment.py:48\u001b[0m, in \u001b[0;36mGraphicDisplay.__init__\u001b[1;34m(self, agent)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=40'>41</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimprovement_count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_moving \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=42'>43</a>\u001b[0m (\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=43'>44</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup,\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=44'>45</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown,\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=45'>46</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft,\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=46'>47</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright,\n\u001b[1;32m---> <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=47'>48</a>\u001b[0m ), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshapes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_images()\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=48'>49</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcanvas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_canvas()\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=49'>50</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_reward(\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mR : 1.0\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter7_2_PolicyIteration\\environment.py:104\u001b[0m, in \u001b[0;36mGraphicDisplay.load_images\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=102'>103</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_images\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=103'>104</a>\u001b[0m     up \u001b[39m=\u001b[39m PhotoImage(Image\u001b[39m.\u001b[39;49mopen(PATH \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m/img/up.png\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mresize((\u001b[39m13\u001b[39m, \u001b[39m13\u001b[39m)))\n\u001b[0;32m    <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=104'>105</a>\u001b[0m     right \u001b[39m=\u001b[39m PhotoImage(Image\u001b[39m.\u001b[39mopen(PATH \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/img/right.png\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mresize((\u001b[39m13\u001b[39m, \u001b[39m13\u001b[39m)))\n\u001b[0;32m    <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_2_PolicyIteration/environment.py?line=105'>106</a>\u001b[0m     left \u001b[39m=\u001b[39m PhotoImage(Image\u001b[39m.\u001b[39mopen(PATH \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/img/left.png\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mresize((\u001b[39m13\u001b[39m, \u001b[39m13\u001b[39m)))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\PIL\\Image.py:2953\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/PIL/Image.py?line=2949'>2950</a>\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/PIL/Image.py?line=2951'>2952</a>\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/PIL/Image.py?line=2952'>2953</a>\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/PIL/Image.py?line=2953'>2954</a>\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/PIL/Image.py?line=2955'>2956</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Selbststudium\\\\Udemy\\\\Udemy_AI_\\\\Chapter7_2_PolicyIteration/img/up.png'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad6c57515631d9bb109eb499b4bb1ec6f506277a5ee28599dfb5ae2805d1ee5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_udemy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
