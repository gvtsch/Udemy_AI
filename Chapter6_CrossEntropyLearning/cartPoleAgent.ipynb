{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole RL\n",
    "Policy erlenen, indem 100 Epochen durchgeführt und die besten 30 gefiltert und zum Erkennen von Mustern genutzt werden. Werden der 100 Epochen, wird die Policy nicht angepasst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        self.env = env\n",
    "        self.observations: int = self.env.observation_space.shape[0] # Anzahl Observation States\n",
    "        self.actions: int = self.env.action_space.n # Anzahl Actions (Hier 2)\n",
    "        self.model = self.get_model()\n",
    "\n",
    "    def get_model(self) -> Sequential:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=100, input_dim=self.observations))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Dense(units=self.actions))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "        model.summary()\n",
    "        model.compile(\n",
    "            optimizer=Adam(learning_rate=0.007),\n",
    "            loss=\"categorical_crossentropy\",\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state: Any) -> Any:\n",
    "        state = state.reshape(1, -1)\n",
    "        policy = self.model(state, training=False).numpy()[0] \n",
    "            # [0.7, 0.3] Wahrschlichkeit für jede Aktion in den Step. In Summe immer =1\n",
    "            # Aktion Links tritt mit 70%iger Wahrscheinlichkeit ein\n",
    "            # Kann hier als Policy angesehen werden\n",
    "        return np.random.choice(self.actions, p=policy)\n",
    "\n",
    "    def get_samples(self, num_episodes: int) -> Tuple[List[float], List[Any]]: \n",
    "        rewards = [0.0 for _ in range(num_episodes)]\n",
    "        episodes: List[Any] = [[] for _ in range(num_episodes)]\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                episodes[episode].append((state, action))\n",
    "                state = new_state\n",
    "                if done:\n",
    "                    rewards[episode] = total_reward\n",
    "                    break\n",
    "        return rewards, episodes\n",
    "\n",
    "    def filter_episodes(\n",
    "        self,\n",
    "        rewards: List[float],\n",
    "        episodes: List[Tuple[Any, Any]],\n",
    "        percentile: float,\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, float]:\n",
    "        reward_bound = np.percentile(rewards, percentile)\n",
    "        x_train = []\n",
    "        y_train = []\n",
    "        for reward, epsiode in zip(rewards, episodes):\n",
    "            if reward >= reward_bound:\n",
    "                observation = [step[0] for step in epsiode]\n",
    "                action = [step[1] for step in epsiode]\n",
    "                x_train.extend(observation)\n",
    "                y_train.extend(action)\n",
    "        x_train = np.asarray(x_train)\n",
    "        y_train = to_categorical(y_train, num_classes=self.actions)\n",
    "        return x_train, y_train, reward_bound\n",
    "\n",
    "\n",
    "    def train(\n",
    "        self, percentile: float, num_iterations: int, num_episodes: int\n",
    "    ) -> Tuple[List[float], List[float]]:\n",
    "        reward_means: List[float] = []\n",
    "        reward_bounds: List[float] = []\n",
    "        for it in range(num_iterations):\n",
    "            rewards, episodes = self.get_samples(num_episodes)\n",
    "            x_train, y_train, reward_bound = self.filter_episodes(\n",
    "                rewards, episodes, percentile\n",
    "            )\n",
    "            self.model.train_on_batch(x=x_train, y=y_train)\n",
    "            reward_mean = np.mean(rewards)\n",
    "            print(\n",
    "                f\"Iteration: {it:2d} \"\n",
    "                f\"Reward Mean: {reward_mean:.4f} \"\n",
    "                f\"Reward Bound: {reward_bound:.4f}\"\n",
    "            )\n",
    "            reward_bounds.append(reward_bound)\n",
    "            reward_means.append(reward_mean)\n",
    "        return reward_means, reward_bounds\n",
    "\n",
    "\n",
    "    def play(self, episodes: int, render: bool = True) -> None:\n",
    "        for episode in range(episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while True:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                action = self.get_action(state)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            print(f\"Episode: {episode} Total Reward: {total_reward}\")\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    agent = Agent(env)\n",
    "    reward_means, reward_bounds = agent.train(percentile=70.0, num_iterations=30, num_episodes=100)\n",
    "    input()\n",
    "        # 30 Episoden mit je 100 Iterationen, die 70% der guten filtern\n",
    "    agent.play(episodes=10)\n",
    "\n",
    "    plt.title(\"Training Performance\")\n",
    "    plt.plot(range(len(reward_means)), reward_means, color=\"red\")\n",
    "    plt.plot(range(len(reward_bounds)), reward_bounds, color=\"blue\")\n",
    "    plt.legend([\"reward_means\", \"reward_bounds\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad6c57515631d9bb109eb499b4bb1ec6f506277a5ee28599dfb5ae2805d1ee5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_udemy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
