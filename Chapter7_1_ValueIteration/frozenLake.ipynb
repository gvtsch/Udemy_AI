{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any\n",
    "\n",
    "from plotting import plotting_fn, action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        self.env = env\n",
    "        self.observations: int = self.env.observation_space.n\n",
    "        self.actions: int = self.env.action_space.n\n",
    "        self.gamma = 0.9\n",
    "        self.state = self.env.reset()\n",
    "        self.S = range(self.observations)\n",
    "        self.A = range(self.actions)\n",
    "        self.rewards = {s: {a: {s_next: 0.0 for s_next in self.S} for a in self.A} for s in self.S}\n",
    "        self.transitions = {s: {a: {s_next: 0.0 for s_next in self.S} for a in self.A} for s in self.S}\n",
    "        self.q_values = {s: {a: 0.0 for a in self.A} for s in self.S}\n",
    "\n",
    "    def get_action(self, state: Any) -> Any:\n",
    "        q_values = list(self.q_values[state].values()) # q_values umformen\n",
    "        action = np.argmax(q_values).astype(int) # höchster q_value finden\n",
    "        return action\n",
    "\n",
    "    def get_random_action(self) -> Any:\n",
    "        action = self.env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def get_samples(self, num_episodes: int) -> None: # Methode um Dicts mit Samples updaten\n",
    "        for _ in range(num_episodes):\n",
    "            action = self.get_random_action()\n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            self.rewards[self.state][action][new_state] = reward\n",
    "            self.transitions[self.state][action][new_state] += 1\n",
    "            if done:\n",
    "                self.state = self.env.reset()\n",
    "            else:\n",
    "                self.state = new_state\n",
    "\n",
    "    def compute_q_values(self) -> None:\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                q_value = 0.0\n",
    "                transitions_dict = self.transitions[s][a]\n",
    "                total_transitions = np.sum(list(transitions_dict.values())).astype(int)\n",
    "                if total_transitions > 0:\n",
    "                    for s_next, count in transitions_dict.items():\n",
    "                        reward = self.rewards[s][a][s_next]\n",
    "                        best_action = self.get_action(s_next)\n",
    "                        q_value += (count/total_transitions) * (reward + self.gamma * self.q_values[s_next][best_action])\n",
    "                    self.q_values[s][a] = q_value\n",
    "\n",
    "    def train(self, num_iterations: int, num_episodes: int) -> None:\n",
    "        self.get_samples(num_episodes=1_000)\n",
    "        for _ in range(num_iterations):\n",
    "            self.get_samples(num_episodes=num_episodes)\n",
    "            self.compute_q_values()\n",
    "            reward_mean = self.play(num_episodes=20, render=False)\n",
    "            if reward_mean >= 0.9: # hat man 90% der Episoden gewonnen?\n",
    "                break\n",
    "\n",
    "    def play(self, num_episodes: int, render: bool = True) -> float:\n",
    "        reward_sum = 0.0\n",
    "        if render:\n",
    "            _, ax = plt.subplots(figsize=(8,)) # Größe der Grafik\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                if render:\n",
    "                    print(f\"Action: {action_map(action)}\")\n",
    "                    plotting_fn(state, ax)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    reward_sum += total_reward\n",
    "                    break\n",
    "\n",
    "            print(f\"Episode: {episode} Total Reward: {total_reward}\")\n",
    "        self.env.close()\n",
    "        return reward_sum / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    env = gym.make(\"FrozenLake-v1\")\n",
    "    agent = Agent(env)\n",
    "    agent.train(num_iterations=10_000, num_episodes=1_000)\n",
    "    agent.play(num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_action() missing 1 required positional argument: 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter7_1_ValueIteration\\frozenLake.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000003?line=0'>1</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000003?line=1'>2</a>\u001b[0m     main()\n",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter7_1_ValueIteration\\frozenLake.ipynb Cell 3'\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000002?line=1'>2</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mFrozenLake-v1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000002?line=2'>3</a>\u001b[0m agent \u001b[39m=\u001b[39m Agent(env)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000002?line=3'>4</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(num_iterations\u001b[39m=\u001b[39;49m\u001b[39m10_000\u001b[39;49m, num_episodes\u001b[39m=\u001b[39;49m\u001b[39m1_000\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000002?line=4'>5</a>\u001b[0m agent\u001b[39m.\u001b[39mplay(num_episodes\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter7_1_ValueIteration\\frozenLake.ipynb Cell 2'\u001b[0m in \u001b[0;36mAgent.train\u001b[1;34m(self, num_iterations, num_episodes)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000001?line=49'>50</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_samples(num_episodes\u001b[39m=\u001b[39mnum_episodes)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000001?line=50'>51</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_q_values()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000001?line=51'>52</a>\u001b[0m reward_mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay(num_episodes\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, render\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000001?line=52'>53</a>\u001b[0m \u001b[39mif\u001b[39;00m reward_mean \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m: \u001b[39m# hat man 90% der Episoden gewonnen?\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000001?line=53'>54</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter7_1_ValueIteration\\frozenLake.ipynb Cell 2'\u001b[0m in \u001b[0;36mAgent.play\u001b[1;34m(self, num_episodes, render)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000001?line=61'>62</a>\u001b[0m total_reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000001?line=63'>64</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000001?line=64'>65</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_action()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000001?line=65'>66</a>\u001b[0m     \u001b[39mif\u001b[39;00m render:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter7_1_ValueIteration/frozenLake.ipynb#ch0000001?line=66'>67</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAction: \u001b[39m\u001b[39m{\u001b[39;00maction_map(action)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: get_action() missing 1 required positional argument: 'state'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad6c57515631d9bb109eb499b4bb1ec6f506277a5ee28599dfb5ae2805d1ee5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_udemy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
