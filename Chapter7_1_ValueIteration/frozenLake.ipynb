{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Any\n",
    "\n",
    "from plotting import plotting_fn, action_map, plotting_q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: gym.Env) -> None:\n",
    "        self.env = env\n",
    "        self.observations: int = self.env.observation_space.n\n",
    "        self.actions: int = self.env.action_space.n\n",
    "        self.gamma = 0.9\n",
    "        self.state = self.env.reset()\n",
    "        self.S = range(self.observations)\n",
    "        self.A = range(self.actions)\n",
    "        self.rewards = {s: {a: {s_next: 0.0 for s_next in self.S} for a in self.A} for s in self.S}\n",
    "        self.transitions = {s: {a: {s_next: 0.0 for s_next in self.S} for a in self.A} for s in self.S}\n",
    "        self.q_values = {s: {a: 0.0 for a in self.A} for s in self.S}\n",
    "\n",
    "    def get_action(self, state: Any) -> Any:\n",
    "        q_values = list(self.q_values[state].values()) # q_values umformen\n",
    "        action = np.argmax(q_values).astype(int) # höchster q_value finden\n",
    "        return action\n",
    "\n",
    "    def get_random_action(self) -> Any:\n",
    "        action = self.env.action_space.sample()\n",
    "        return action\n",
    "\n",
    "    def get_samples(self, num_episodes: int) -> None: # Methode um Dicts mit Samples updaten\n",
    "        for _ in range(num_episodes):\n",
    "            action = self.get_random_action()\n",
    "            new_state, reward, done, _ = self.env.step(action)\n",
    "            self.rewards[self.state][action][new_state] = reward\n",
    "            self.transitions[self.state][action][new_state] += 1\n",
    "            if done:\n",
    "                self.state = self.env.reset()\n",
    "            else:\n",
    "                self.state = new_state\n",
    "\n",
    "    def compute_q_values(self) -> None:\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                q_value = 0.0\n",
    "                transitions_dict = self.transitions[s][a]\n",
    "                total_transitions = np.sum(list(transitions_dict.values())).astype(int)\n",
    "                if total_transitions > 0:\n",
    "                    for s_next, count in transitions_dict.items():\n",
    "                        reward = self.rewards[s][a][s_next]\n",
    "                        best_action = self.get_action(s_next)\n",
    "                        q_value += (count/total_transitions) * (reward + self.gamma * self.q_values[s_next][best_action])\n",
    "                    self.q_values[s][a] = q_value\n",
    "\n",
    "    def train(self, num_iterations: int, num_episodes: int) -> None:\n",
    "        self.get_samples(num_episodes=1_000)\n",
    "        for _ in range(num_iterations):\n",
    "            self.get_samples(num_episodes=num_episodes)\n",
    "            self.compute_q_values()\n",
    "            reward_mean = self.play(num_episodes=20, render=False)\n",
    "            if reward_mean >= 0.9: # hat man 90% der Episoden gewonnen?\n",
    "                break\n",
    "\n",
    "    def play(self, num_episodes: int, render: bool = True) -> float:\n",
    "        reward_sum = 0.0\n",
    "        if render:\n",
    "            _, ax = plt.subplots(figsize=(8,8)) # Größe der Grafik\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0.0\n",
    "\n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                if render:\n",
    "                    print(f\"Action: {action_map(action)}\")\n",
    "                    plotting_q_values(state, action, self.q_values, ax)\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done:\n",
    "                    reward_sum += total_reward\n",
    "                    break\n",
    "\n",
    "            print(f\"Episode: {episode} Total Reward: {total_reward}\")\n",
    "        self.env.close()\n",
    "        return reward_sum / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    env = gym.make(\"FrozenLake-v1\")\n",
    "    agent = Agent(env)\n",
    "    agent.train(num_iterations=10_000, num_episodes=1_000)\n",
    "    agent.play(num_episodes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad6c57515631d9bb109eb499b4bb1ec6f506277a5ee28599dfb5ae2805d1ee5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_udemy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
