{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import random\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartPoleDqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.path.abspath(\"C:/Selbststudium/Udemy/Udemy_AI_\")\n",
    "MODELS_PATH = os.path.join(PROJECT_PATH, \"models\")\n",
    "MODEL_PATH = os.path.join(MODELS_PATH, \"dqn_cartpole.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: gym.Env):\n",
    "        # DQN Env Variables\n",
    "        self.env = env\n",
    "        self.observations = self.env.observation_space.shape\n",
    "        self.actions = self.env.action_space.n\n",
    "        # DQN Agent Variables\n",
    "        self.replay_buffer_size = 50_000\n",
    "        self.train_start = 1_000 \n",
    "            # Ab wie vielen gemachten Spielzügen im Replay-Buffer soll mit dem Training begonnen werden?\n",
    "            # Das Netzwerk wird so nach 1_000 Spielzügen eingeschaltet\n",
    "        self.memory = collections.deque(maxlen=self.replay_buffer_size)\n",
    "            # Eine Liste an der an beiden Seiten etwas ändern kann\n",
    "            # Ist der Speicher erstmal voll, werden die Daten von rechts nach links verschoben, \n",
    "            # bzw. die ältesten Daten werden zugunsten der neuen Daten gelöscht\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "            # Wie viel Prozent der der Aktionen sollen schon zu Beginn zufällig gewählt sein?\n",
    "            # Dieser Wert wird im Laufe des Trainings reduziert bis epsilon_min\n",
    "        self.epsilon_min = 0.01\n",
    "            # Minimaler Prozentsatz um zufällige Aktionen auszuführen\n",
    "            # In einem Prozent der Fälle wollen wir noch eine zufällige Aktion haben\n",
    "        self.epsilon_decay = 0.999 \n",
    "            # Je näher an der 1, desto mehr Spielzüge benötigt man,\n",
    "            # um mit epsilon an epsilon_min anzukommen\n",
    "        # DQN Network Variables\n",
    "        self.state_shape = self.observations\n",
    "        self.learning_rate = 1e-3\n",
    "        self.dqn = DQN(\n",
    "            self.state_shape,\n",
    "            self.actions,\n",
    "            self.learning_rate\n",
    "        )\n",
    "        self.target_dqn = DQN(\n",
    "            self.state_shape,\n",
    "            self.actions,\n",
    "            self.learning_rate\n",
    "        )\n",
    "        self.target_dqn.update_model(self.dqn)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon: \n",
    "            return np.random.randint(self.actions)\n",
    "        else:\n",
    "            return np.argmax(self.dqn(state)) # Die Aktion mit dem höchsten q-Value\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        last_rewards: Deque = collections.deque(maxlen=10)\n",
    "        best_reward_mean = 0.0\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            total_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, newshape=(1, -1)).astype(np.float32) # Wieder für TF\n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action) # ausführen des steps\n",
    "                next_state = np.reshape(next_state, newshape=(1, -1)).astype(np.float32)\n",
    "                if done and total_reward < 500: # reward = 500 --> Gewonnen\n",
    "                    reward = -100 # Verloren \"böse bestrafen\"\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                self.replay()\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    if total_reward < 500:\n",
    "                        total_reward += 100\n",
    "                    self.target_dqn.update_model(self.dqn)\n",
    "                    print(f\"Episode: {episode} --- Reward: {reward} --- Epsilon: {self.epsilon}\")\n",
    "                    current_reward_mean = np.mean(last_rewards)\n",
    "                    if current_reward_mean > best_reward_mean:\n",
    "                        best_reward_mean = current_reward_mean\n",
    "                        self.dqn.save_model(MODEL_PATH)\n",
    "                    break\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done ):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, states_next, dones = zip(*minibatch)\n",
    "        \n",
    "        states = np.concatenate(states).astype(np.float32)\n",
    "        states_next = np.concatenate(states_next).astype(np.float32)\n",
    "\n",
    "        q_values = self.dqn(states)\n",
    "        q_values_next = self.target_dqn(states_next)\n",
    "\n",
    "        # Nun folgt die Umsetzung der theoretischen Formel:\n",
    "        for i in range(self.batch_size):\n",
    "            a = actions[i]\n",
    "            done = dones[i]\n",
    "            if done:\n",
    "                q_values[i][a] = rewards[i]\n",
    "            else: \n",
    "                q_values[i][a] = rewards[i] + self.gamma * np.max(q_values_next[i])\n",
    "\n",
    "        self.dqn.fit(states, q_values) \n",
    "            # Training des Netzwerks auf den aktualisierten q_values, \n",
    "            # basierend auf den Aktionen, welche ausgeführt wurden\n",
    "\n",
    "    def play(self, num_episodes, render=True):\n",
    "        self.dqn.load_model(MODEL_PATH)\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            total_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, newshape=(1, -1)).astype(np.float32) # Wieder für TF\n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action) # ausführen des steps\n",
    "                next_state = np.reshape(next_state, newshape=(1, -1)).astpye(np.float32)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    print(f\"Episode: {episode} --- Reward: {reward} --- Epsilon: {self.epsilon}\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 --- Reward: -100 --- Epsilon: 0.9811700348643991\n",
      "Episode: 2 --- Reward: -100 --- Epsilon: 0.9684910757595269\n",
      "Episode: 3 --- Reward: -100 --- Epsilon: 0.9550199818235596\n",
      "Episode: 4 --- Reward: -100 --- Epsilon: 0.9407945259609451\n",
      "Episode: 5 --- Reward: -100 --- Epsilon: 0.9295668775782806\n",
      "Episode: 6 --- Reward: -100 --- Epsilon: 0.9175547491935327\n",
      "Episode: 7 --- Reward: -100 --- Epsilon: 0.9029834676116293\n",
      "Episode: 8 --- Reward: -100 --- Epsilon: 0.8931001648639156\n",
      "Episode: 9 --- Reward: -100 --- Epsilon: 0.8762831198969288\n",
      "Episode: 10 --- Reward: -100 --- Epsilon: 0.8546369224228781\n",
      "Episode: 11 --- Reward: -100 --- Epsilon: 0.8419067177676068\n",
      "Episode: 12 --- Reward: -100 --- Epsilon: 0.8153791427799216\n",
      "Episode: 13 --- Reward: -100 --- Epsilon: 0.8008264083574369\n",
      "Episode: 14 --- Reward: -100 --- Epsilon: 0.7771467460721305\n",
      "Episode: 15 --- Reward: -100 --- Epsilon: 0.7526596881264136\n",
      "Episode: 16 --- Reward: -100 --- Epsilon: 0.7399663251239436\n",
      "Episode: 17 --- Reward: -100 --- Epsilon: 0.7152181973314037\n",
      "Episode: 18 --- Reward: -100 --- Epsilon: 0.704564697832001\n",
      "Episode: 19 --- Reward: -100 --- Epsilon: 0.6947646516921667\n",
      "Episode: 20 --- Reward: -100 --- Epsilon: 0.6769247732130653\n",
      "Episode: 21 --- Reward: -100 --- Epsilon: 0.6648431464805192\n",
      "Episode: 22 --- Reward: -100 --- Epsilon: 0.6582245533155777\n",
      "Episode: 23 --- Reward: -100 --- Epsilon: 0.6503691570122084\n",
      "Episode: 24 --- Reward: -100 --- Epsilon: 0.6426075087326283\n",
      "Episode: 25 --- Reward: -100 --- Epsilon: 0.6336692476264985\n",
      "Episode: 26 --- Reward: -100 --- Epsilon: 0.6155478087077623\n",
      "Episode: 27 --- Reward: -100 --- Epsilon: 0.6063789448611848\n",
      "Episode: 28 --- Reward: -100 --- Epsilon: 0.5997420274569785\n",
      "Episode: 29 --- Reward: -100 --- Epsilon: 0.5925845744544587\n",
      "Episode: 30 --- Reward: -100 --- Epsilon: 0.5860986387058792\n",
      "Episode: 31 --- Reward: -100 --- Epsilon: 0.573338959119908\n",
      "Episode: 32 --- Reward: -100 --- Epsilon: 0.5670636698003494\n",
      "Episode: 33 --- Reward: -100 --- Epsilon: 0.5608570645557482\n",
      "Episode: 34 --- Reward: -100 --- Epsilon: 0.5547183916275941\n",
      "Episode: 35 --- Reward: -100 --- Epsilon: 0.5470026121551156\n",
      "Episode: 36 --- Reward: -100 --- Epsilon: 0.5313596336427661\n",
      "Episode: 37 --- Reward: -100 --- Epsilon: 0.5250182711383566\n",
      "Episode: 38 --- Reward: -100 --- Epsilon: 0.5177156014229363\n",
      "Episode: 39 --- Reward: -100 --- Epsilon: 0.5125616805934888\n",
      "Episode: 40 --- Reward: -100 --- Epsilon: 0.5039174930867489\n",
      "Episode: 41 --- Reward: -100 --- Epsilon: 0.4974057274803321\n",
      "Episode: 42 --- Reward: -100 --- Epsilon: 0.4904871306580321\n",
      "Episode: 43 --- Reward: -100 --- Epsilon: 0.48173300809637676\n",
      "Episode: 44 --- Reward: -100 --- Epsilon: 0.47360873621294336\n",
      "Episode: 45 --- Reward: -100 --- Epsilon: 0.4660875651208925\n",
      "Episode: 46 --- Reward: -100 --- Epsilon: 0.4600646486360102\n",
      "Episode: 47 --- Reward: -100 --- Epsilon: 0.45594059053811675\n",
      "Episode: 48 --- Reward: -100 --- Epsilon: 0.44869999946146477\n",
      "Episode: 49 --- Reward: -100 --- Epsilon: 0.44245886829040726\n",
      "Episode: 50 --- Reward: -100 --- Epsilon: 0.43412738322656147\n",
      "Episode: 51 --- Reward: -100 --- Epsilon: 0.4302358289512637\n",
      "Episode: 52 --- Reward: -100 --- Epsilon: 0.42382727110771445\n",
      "Episode: 53 --- Reward: -100 --- Epsilon: 0.41460032563814003\n",
      "Episode: 54 --- Reward: -100 --- Epsilon: 0.40924274195045723\n",
      "Episode: 55 --- Reward: -100 --- Epsilon: 0.3943701631993207\n",
      "Episode: 56 --- Reward: -100 --- Epsilon: 0.38849584071717547\n",
      "Episode: 57 --- Reward: -100 --- Epsilon: 0.38347558663089293\n",
      "Episode: 58 --- Reward: -100 --- Epsilon: 0.3777635438089284\n",
      "Episode: 59 --- Reward: -100 --- Epsilon: 0.37400286247792053\n",
      "Episode: 60 --- Reward: -100 --- Epsilon: 0.36990933956087574\n",
      "Episode: 61 --- Reward: -100 --- Epsilon: 0.36512926554500874\n",
      "Episode: 62 --- Reward: -100 --- Epsilon: 0.3568230267186975\n",
      "Episode: 63 --- Reward: -100 --- Epsilon: 0.35150798586723914\n",
      "Episode: 64 --- Reward: -100 --- Epsilon: 0.3483570387995002\n",
      "Episode: 65 --- Reward: -100 --- Epsilon: 0.34488910274847373\n",
      "Episode: 66 --- Reward: -100 --- Epsilon: 0.3414556904162602\n",
      "Episode: 67 --- Reward: -100 --- Epsilon: 0.33771840165698813\n",
      "Episode: 68 --- Reward: -100 --- Epsilon: 0.33435637451312045\n",
      "Episode: 69 --- Reward: -100 --- Epsilon: 0.3293759846379912\n",
      "Episode: 70 --- Reward: -100 --- Epsilon: 0.325770910247633\n",
      "Episode: 71 --- Reward: -100 --- Epsilon: 0.32188308869613635\n",
      "Episode: 72 --- Reward: -100 --- Epsilon: 0.31708849407160733\n",
      "Episode: 73 --- Reward: -100 --- Epsilon: 0.31330429038059615\n",
      "Episode: 74 --- Reward: -100 --- Epsilon: 0.30987512333041833\n",
      "Episode: 75 --- Reward: -100 --- Epsilon: 0.30709737673437937\n",
      "Episode: 76 --- Reward: -100 --- Epsilon: 0.30373614537615473\n",
      "Episode: 77 --- Reward: -100 --- Epsilon: 0.30011129154539284\n",
      "Episode: 78 --- Reward: -100 --- Epsilon: 0.2968265240399636\n",
      "Episode: 79 --- Reward: -100 --- Epsilon: 0.2935777088557856\n",
      "Episode: 80 --- Reward: -100 --- Epsilon: 0.28978401394825715\n",
      "Episode: 81 --- Reward: -100 --- Epsilon: 0.2871863657418441\n",
      "Episode: 82 --- Reward: -100 --- Epsilon: 0.28319178633180314\n",
      "Episode: 83 --- Reward: -100 --- Epsilon: 0.2803725781752547\n",
      "Episode: 84 --- Reward: -100 --- Epsilon: 0.2753685408440802\n",
      "Episode: 85 --- Reward: -100 --- Epsilon: 0.27126680151057403\n",
      "Episode: 86 --- Reward: -100 --- Epsilon: 0.25674293352423394\n",
      "Episode: 87 --- Reward: -100 --- Epsilon: 0.24519463170764128\n",
      "Episode: 88 --- Reward: -100 --- Epsilon: 0.23723134382036148\n",
      "Episode: 89 --- Reward: -100 --- Epsilon: 0.22769688338723304\n",
      "Episode: 90 --- Reward: -100 --- Epsilon: 0.2209641067882625\n",
      "Episode: 91 --- Reward: -100 --- Epsilon: 0.20955238543722926\n",
      "Episode: 92 --- Reward: -100 --- Epsilon: 0.197738359181609\n",
      "Episode: 93 --- Reward: -100 --- Epsilon: 0.19266091563311397\n",
      "Episode: 94 --- Reward: -100 --- Epsilon: 0.1748420854254404\n",
      "Episode: 95 --- Reward: -100 --- Epsilon: 0.16564652979915298\n",
      "Episode: 96 --- Reward: -100 --- Epsilon: 0.15459698847728787\n",
      "Episode: 97 --- Reward: -100 --- Epsilon: 0.1169424652028001\n",
      "Episode: 98 --- Reward: -100 --- Epsilon: 0.09994334856146549\n",
      "Episode: 99 --- Reward: -100 --- Epsilon: 0.08592956600102684\n",
      "Episode: 100 --- Reward: -100 --- Epsilon: 0.06902152328662714\n",
      "Episode: 101 --- Reward: -100 --- Epsilon: 0.06090747113633568\n",
      "Episode: 102 --- Reward: -100 --- Epsilon: 0.05412503357177137\n",
      "Episode: 103 --- Reward: -100 --- Epsilon: 0.04996170339735633\n",
      "Episode: 104 --- Reward: -100 --- Epsilon: 0.04484458044570669\n",
      "Episode: 105 --- Reward: -100 --- Epsilon: 0.037044029073993116\n",
      "Episode: 106 --- Reward: -100 --- Epsilon: 0.025840087369154233\n",
      "Episode: 107 --- Reward: -100 --- Epsilon: 0.02370969488514328\n",
      "Episode: 108 --- Reward: -100 --- Epsilon: 0.022395398471200194\n",
      "Episode: 109 --- Reward: -100 --- Epsilon: 0.015779007156688403\n",
      "Episode: 110 --- Reward: -100 --- Epsilon: 0.012213627178383235\n",
      "Episode: 111 --- Reward: -100 --- Epsilon: 0.01059604971056349\n",
      "Episode: 112 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 113 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 114 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 115 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 116 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 117 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 118 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 119 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 120 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 121 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 122 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 123 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 124 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 125 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 126 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 127 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 128 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 129 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 130 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 131 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 132 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 133 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 134 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 135 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 136 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 137 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 138 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 139 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 140 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 141 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 142 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 143 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 144 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 145 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 146 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 147 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 148 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 149 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 150 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 151 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 152 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 153 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 154 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 155 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 156 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 157 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 158 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 159 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 160 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 161 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 162 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 163 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 164 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 165 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 166 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 167 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 168 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 169 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 170 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 171 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 172 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 173 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 174 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 175 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 176 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 177 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 178 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 179 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 180 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 181 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 182 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 183 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 184 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 185 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 186 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 187 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 188 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 189 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 190 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 191 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 192 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 193 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 194 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 195 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 196 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 197 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 198 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 199 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 200 --- Reward: -100 --- Epsilon: 0.009998671593271896\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'C:\\Selbststudium\\Udemy\\Udemy_AI_\\models\\dqn_cartpole.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter10_DeepQNetworks\\cartPoleDqnAgent.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000004?line=3'>4</a>\u001b[0m agent\u001b[39m.\u001b[39mtrain(num_episodes\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000004?line=4'>5</a>\u001b[0m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPlay?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000004?line=5'>6</a>\u001b[0m agent\u001b[39m.\u001b[39;49mplay(num_episodes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, render\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter10_DeepQNetworks\\cartPoleDqnAgent.ipynb Cell 4'\u001b[0m in \u001b[0;36mAgent.play\u001b[1;34m(self, num_episodes, render)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=106'>107</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplay\u001b[39m(\u001b[39mself\u001b[39m, num_episodes, render\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=107'>108</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdqn\u001b[39m.\u001b[39;49mload_model(MODEL_PATH)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=108'>109</a>\u001b[0m     \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_episodes \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=109'>110</a>\u001b[0m         total_reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter10_DeepQNetworks\\cartPoleDqn.py:48\u001b[0m, in \u001b[0;36mDQN.load_model\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqn.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[1;32m---> <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqn.py?line=47'>48</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minternal_model\u001b[39m.\u001b[39;49mload_weights(path)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\h5py\\_hl\\files.py:424\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=421'>422</a>\u001b[0m \u001b[39mwith\u001b[39;00m phil:\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=422'>423</a>\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m--> <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=423'>424</a>\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=424'>425</a>\u001b[0m                    fapl, fcpl\u001b[39m=\u001b[39mmake_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=425'>426</a>\u001b[0m                    fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold),\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=426'>427</a>\u001b[0m                    swmr\u001b[39m=\u001b[39mswmr)\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=428'>429</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=429'>430</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\h5py\\_hl\\files.py:190\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=187'>188</a>\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=188'>189</a>\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=189'>190</a>\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=190'>191</a>\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=191'>192</a>\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:96\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'C:\\Selbststudium\\Udemy\\Udemy_AI_\\models\\dqn_cartpole.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    agent = Agent(env)\n",
    "    agent.train(num_episodes=20)\n",
    "    input(\"Play?\")\n",
    "    agent.play(num_episodes=10, render=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad6c57515631d9bb109eb499b4bb1ec6f506277a5ee28599dfb5ae2805d1ee5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_udemy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
