{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import random\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cartPoleDqn import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_PATH = os.path.abspath(\"C:/Selbststudium/Udemy/Udemy_AI_\")\n",
    "MODELS_PATH = os.path.join(PROJECT_PATH, \"models\")\n",
    "MODEL_PATH = os.path.join(MODELS_PATH, \"dqn_cartpole.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env: gym.Env):\n",
    "        # DQN Env Variables\n",
    "        self.env = env\n",
    "        self.observations = self.env.observation_space.shape\n",
    "        self.actions = self.env.action_space.n\n",
    "        # DQN Agent Variables\n",
    "        self.replay_buffer_size = 50_000\n",
    "        self.train_start = 1_000 \n",
    "            # Ab wie vielen gemachten Spielzügen im Replay-Buffer soll mit dem Training begonnen werden?\n",
    "            # Das Netzwerk wird so nach 1_000 Spielzügen eingeschaltet\n",
    "        self.memory = collections.deque(maxlen=self.replay_buffer_size)\n",
    "            # Eine Liste an der an beiden Seiten etwas ändern kann\n",
    "            # Ist der Speicher erstmal voll, werden die Daten von rechts nach links verschoben, \n",
    "            # bzw. die ältesten Daten werden zugunsten der neuen Daten gelöscht\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "            # Wie viel Prozent der der Aktionen sollen schon zu Beginn zufällig gewählt sein?\n",
    "            # Dieser Wert wird im Laufe des Trainings reduziert bis epsilon_min\n",
    "        self.epsilon_min = 0.01\n",
    "            # Minimaler Prozentsatz um zufällige Aktionen auszuführen\n",
    "            # In einem Prozent der Fälle wollen wir noch eine zufällige Aktion haben\n",
    "        self.epsilon_decay = 0.999 \n",
    "            # Je näher an der 1, desto mehr Spielzüge benötigt man,\n",
    "            # um mit epsilon an epsilon_min anzukommen\n",
    "        # DQN Network Variables\n",
    "        self.state_shape = self.observations\n",
    "        self.learning_rate = 1e-3\n",
    "        self.dqn = DQN(\n",
    "            self.state_shape,\n",
    "            self.actions,\n",
    "            self.learning_rate\n",
    "        )\n",
    "        self.target_dqn = DQN(\n",
    "            self.state_shape,\n",
    "            self.actions,\n",
    "            self.learning_rate\n",
    "        )\n",
    "        self.target_dqn.update_model(self.dqn)\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() <= self.epsilon: \n",
    "            return np.random.randint(self.actions)\n",
    "        else:\n",
    "            return np.argmax(self.dqn(state)) # Die Aktion mit dem höchsten q-Value\n",
    "\n",
    "    def train(self, num_episodes):\n",
    "        last_rewards: Deque = collections.deque(maxlen=10)\n",
    "        best_reward_mean = 0.0\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            total_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, newshape=(1, -1)).astype(np.float32) # Wieder für TF\n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action) # ausführen des steps\n",
    "                next_state = np.reshape(next_state, newshape=(1, -1)).astype(np.float32)\n",
    "                if done and total_reward < 500: # reward = 500 --> Gewonnen\n",
    "                    reward = -100 # Verloren \"böse bestrafen\"\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "                self.replay()\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    if total_reward < 500:\n",
    "                        total_reward += 100\n",
    "                    self.target_dqn.update_model(self.dqn)\n",
    "                    print(f\"Episode: {episode} --- Reward: {reward} --- Epsilon: {self.epsilon}\")\n",
    "                    current_reward_mean = np.mean(last_rewards)\n",
    "                    if current_reward_mean > best_reward_mean:\n",
    "                        best_reward_mean = current_reward_mean\n",
    "                        self.dqn.save_model(MODEL_PATH)\n",
    "                    break\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done ):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, states_next, dones = zip(*minibatch)\n",
    "        \n",
    "        states = np.concatenate(states).astype(np.float32)\n",
    "        states_next = np.concatenate(states_next).astype(np.float32)\n",
    "\n",
    "        q_values = self.dqn(states)\n",
    "        q_values_next = self.target_dqn(states_next)\n",
    "\n",
    "        # Nun folgt die Umsetzung der theoretischen Formel:\n",
    "        for i in range(self.batch_size):\n",
    "            a = actions[i]\n",
    "            done = dones[i]\n",
    "            if done:\n",
    "                q_values[i][a] = rewards[i]\n",
    "            else: \n",
    "                q_values[i][a] = rewards[i] + self.gamma * np.max(q_values_next[i])\n",
    "\n",
    "        self.dqn.fit(states, q_values) \n",
    "            # Training des Netzwerks auf den aktualisierten q_values, \n",
    "            # basierend auf den Aktionen, welche ausgeführt wurden\n",
    "\n",
    "    def play(self, num_episodes, render=True):\n",
    "        self.dqn.load_model(MODEL_PATH)\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            total_reward = 0.0\n",
    "            state = self.env.reset()\n",
    "            state = np.reshape(state, newshape=(1, -1)).astype(np.float32) # Wieder für TF\n",
    "            while True:\n",
    "                action = self.get_action(state)\n",
    "                next_state, reward, done, _ = self.env.step(action) # ausführen des steps\n",
    "                next_state = np.reshape(next_state, newshape=(1, -1)).astpye(np.float32)\n",
    "                total_reward += reward\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    print(f\"Episode: {episode} --- Reward: {reward} --- Epsilon: {self.epsilon}\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1 --- Reward: -100 --- Epsilon: 0.9762739865836303\n",
      "Episode: 2 --- Reward: -100 --- Epsilon: 0.9588496310845509\n",
      "Episode: 3 --- Reward: -100 --- Epsilon: 0.9240034012385749\n",
      "Episode: 4 --- Reward: -100 --- Epsilon: 0.9011784036598737\n",
      "Episode: 5 --- Reward: -100 --- Epsilon: 0.8913148576343527\n",
      "Episode: 6 --- Reward: -100 --- Epsilon: 0.862367254825433\n",
      "Episode: 7 --- Reward: -100 --- Epsilon: 0.8064546837933355\n",
      "Episode: 8 --- Reward: -100 --- Epsilon: 0.7936477332643059\n",
      "Episode: 9 --- Reward: -100 --- Epsilon: 0.7818259902485653\n",
      "Episode: 10 --- Reward: -100 --- Epsilon: 0.750403966288439\n",
      "Episode: 11 --- Reward: -100 --- Epsilon: 0.7399663251239436\n",
      "Episode: 12 --- Reward: -100 --- Epsilon: 0.7304042691684477\n",
      "Episode: 13 --- Reward: -100 --- Epsilon: 0.7202448110511043\n",
      "Episode: 14 --- Reward: -100 --- Epsilon: 0.7109376021267352\n",
      "Episode: 15 --- Reward: -100 --- Epsilon: 0.6885367230816204\n",
      "Episode: 16 --- Reward: -100 --- Epsilon: 0.674896028990821\n",
      "Episode: 17 --- Reward: -100 --- Epsilon: 0.6668416716372578\n",
      "Episode: 18 --- Reward: -100 --- Epsilon: 0.647123808418277\n",
      "Episode: 19 --- Reward: -100 --- Epsilon: 0.6400409317729626\n",
      "Episode: 20 --- Reward: -100 --- Epsilon: 0.6330355783788719\n",
      "Episode: 21 --- Reward: -100 --- Epsilon: 0.6248553120386914\n",
      "Episode: 22 --- Reward: -100 --- Epsilon: 0.6124762189899097\n",
      "Episode: 23 --- Reward: -100 --- Epsilon: 0.6033531078655693\n",
      "Episode: 24 --- Reward: -100 --- Epsilon: 0.5855125400671733\n",
      "Episode: 25 --- Reward: -100 --- Epsilon: 0.5670636698003494\n",
      "Episode: 26 --- Reward: -100 --- Epsilon: 0.5563858806683429\n",
      "Episode: 27 --- Reward: -100 --- Epsilon: 0.5491961035890855\n",
      "Episode: 28 --- Reward: -100 --- Epsilon: 0.5308282740091232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gutsc\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\gutsc\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\numpy\\core\\_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 29 --- Reward: -100 --- Epsilon: 0.5234447908547367\n",
      "Episode: 30 --- Reward: -100 --- Epsilon: 0.5171978858215134\n",
      "Episode: 31 --- Reward: -100 --- Epsilon: 0.5100039926842729\n",
      "Episode: 32 --- Reward: -100 --- Epsilon: 0.5004006352830107\n",
      "Episode: 33 --- Reward: -100 --- Epsilon: 0.4959150020176678\n",
      "Episode: 34 --- Reward: -100 --- Epsilon: 0.4895066468838467\n",
      "Episode: 35 --- Reward: -100 --- Epsilon: 0.4793291555710539\n",
      "Episode: 36 --- Reward: -100 --- Epsilon: 0.47408281903197536\n",
      "Episode: 37 --- Reward: -100 --- Epsilon: 0.4637617835123936\n",
      "Episode: 38 --- Reward: -100 --- Epsilon: 0.452758565214779\n",
      "Episode: 39 --- Reward: -100 --- Epsilon: 0.44735524511437874\n",
      "Episode: 40 --- Reward: -100 --- Epsilon: 0.4429017700604677\n",
      "Episode: 41 --- Reward: -100 --- Epsilon: 0.4354323744883354\n",
      "Episode: 42 --- Reward: -100 --- Epsilon: 0.43066649544671043\n",
      "Episode: 43 --- Reward: -100 --- Epsilon: 0.4204484963977607\n",
      "Episode: 44 --- Reward: -100 --- Epsilon: 0.41626288125048977\n",
      "Episode: 45 --- Reward: -100 --- Epsilon: 0.3853987301463841\n",
      "Episode: 46 --- Reward: -100 --- Epsilon: 0.38118047758114515\n",
      "Episode: 47 --- Reward: -100 --- Epsilon: 0.3777635438089284\n",
      "Episode: 48 --- Reward: -100 --- Epsilon: 0.3736288596154426\n",
      "Episode: 49 --- Reward: -100 --- Epsilon: 0.3702796191800558\n",
      "Episode: 50 --- Reward: -100 --- Epsilon: 0.3654947603053141\n",
      "Episode: 51 --- Reward: -100 --- Epsilon: 0.36077173274200636\n",
      "Episode: 52 --- Reward: -100 --- Epsilon: 0.35753774467029337\n",
      "Episode: 53 --- Reward: -100 --- Epsilon: 0.35397841359256427\n",
      "Episode: 54 --- Reward: -100 --- Epsilon: 0.35045451608208705\n",
      "Episode: 55 --- Reward: -100 --- Epsilon: 0.34488910274847373\n",
      "Episode: 56 --- Reward: -100 --- Epsilon: 0.34111423472584396\n",
      "Episode: 57 --- Reward: -100 --- Epsilon: 0.336033183457224\n",
      "Episode: 58 --- Reward: -100 --- Epsilon: 0.3280604556381725\n",
      "Episode: 59 --- Reward: -100 --- Epsilon: 0.3034324092307786\n",
      "Episode: 60 --- Reward: -100 --- Epsilon: 0.2929908470157829\n",
      "Episode: 61 --- Reward: -100 --- Epsilon: 0.2874738395814255\n",
      "Episode: 62 --- Reward: -100 --- Epsilon: 0.28375902061401054\n",
      "Episode: 63 --- Reward: -100 --- Epsilon: 0.2806532314066613\n",
      "Episode: 64 --- Reward: -100 --- Epsilon: 0.2770265502839066\n",
      "Episode: 65 --- Reward: -100 --- Epsilon: 0.2739944490729594\n",
      "Episode: 66 --- Reward: -100 --- Epsilon: 0.27153833985042447\n",
      "Episode: 67 --- Reward: -100 --- Epsilon: 0.2682977416984114\n",
      "Episode: 68 --- Reward: -100 --- Epsilon: 0.26562680554034973\n",
      "Episode: 69 --- Reward: -100 --- Epsilon: 0.2629824588716937\n",
      "Episode: 70 --- Reward: -100 --- Epsilon: 0.259324540388352\n",
      "Episode: 71 --- Reward: -100 --- Epsilon: 0.2569999334576916\n",
      "Episode: 72 --- Reward: -100 --- Epsilon: 0.25367890699899504\n",
      "Episode: 73 --- Reward: -100 --- Epsilon: 0.25115350309155987\n",
      "Episode: 74 --- Reward: -100 --- Epsilon: 0.2481561820560168\n",
      "Episode: 75 --- Reward: -100 --- Epsilon: 0.24470448763885772\n",
      "Episode: 76 --- Reward: -100 --- Epsilon: 0.24178413056932346\n",
      "Episode: 77 --- Reward: -100 --- Epsilon: 0.2388986256820923\n",
      "Episode: 78 --- Reward: -100 --- Epsilon: 0.2365203612457005\n",
      "Episode: 79 --- Reward: -100 --- Epsilon: 0.2346348076972527\n",
      "Episode: 80 --- Reward: -100 --- Epsilon: 0.23206669108958414\n",
      "Episode: 81 --- Reward: -100 --- Epsilon: 0.22998642580626621\n",
      "Episode: 82 --- Reward: -100 --- Epsilon: 0.22792480819542846\n",
      "Episode: 83 --- Reward: -100 --- Epsilon: 0.22588167109777058\n",
      "Episode: 84 --- Reward: -100 --- Epsilon: 0.22251706113826017\n",
      "Episode: 85 --- Reward: -100 --- Epsilon: 0.21964163219316574\n",
      "Episode: 86 --- Reward: -100 --- Epsilon: 0.21485991685474995\n",
      "Episode: 87 --- Reward: -100 --- Epsilon: 0.207881822100247\n",
      "Episode: 88 --- Reward: -100 --- Epsilon: 0.2029496152000772\n",
      "Episode: 89 --- Reward: -100 --- Epsilon: 0.19694859138428197\n",
      "Episode: 90 --- Reward: -100 --- Epsilon: 0.18808984016399252\n",
      "Episode: 91 --- Reward: -100 --- Epsilon: 0.1817991871399145\n",
      "Episode: 92 --- Reward: -100 --- Epsilon: 0.1731015115206527\n",
      "Episode: 93 --- Reward: -100 --- Epsilon: 0.1649849368967147\n",
      "Episode: 94 --- Reward: -100 --- Epsilon: 0.14823526536822987\n",
      "Episode: 95 --- Reward: -100 --- Epsilon: 0.13973826300053682\n",
      "Episode: 96 --- Reward: -100 --- Epsilon: 0.13159658948768352\n",
      "Episode: 97 --- Reward: -100 --- Epsilon: 0.12530071728950842\n",
      "Episode: 98 --- Reward: -100 --- Epsilon: 0.11405371881775066\n",
      "Episode: 99 --- Reward: -100 --- Epsilon: 0.09631123693243965\n",
      "Episode: 100 --- Reward: -100 --- Epsilon: 0.09133723000062824\n",
      "Episode: 101 --- Reward: -100 --- Epsilon: 0.08389079996522364\n",
      "Episode: 102 --- Reward: -100 --- Epsilon: 0.0812474911046467\n",
      "Episode: 103 --- Reward: -100 --- Epsilon: 0.07751538450049153\n",
      "Episode: 104 --- Reward: -100 --- Epsilon: 0.07299904846428458\n",
      "Episode: 105 --- Reward: -100 --- Epsilon: 0.06826607134844892\n",
      "Episode: 106 --- Reward: -100 --- Epsilon: 0.04681596410987613\n",
      "Episode: 107 --- Reward: -100 --- Epsilon: 0.03961234065868409\n",
      "Episode: 108 --- Reward: -100 --- Epsilon: 0.02510112210191291\n",
      "Episode: 109 --- Reward: -100 --- Epsilon: 0.02305470850320012\n",
      "Episode: 110 --- Reward: -100 --- Epsilon: 0.0210273504213107\n",
      "Episode: 111 --- Reward: -100 --- Epsilon: 0.01517517520633974\n",
      "Episode: 112 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 113 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 114 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 115 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 116 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 117 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 118 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 119 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 120 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 121 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 122 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 123 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 124 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 125 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 126 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 127 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 128 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 129 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 130 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 131 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 132 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 133 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 134 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 135 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 136 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 137 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 138 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 139 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 140 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 141 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 142 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 143 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 144 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 145 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 146 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 147 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 148 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 149 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 150 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 151 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 152 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 153 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 154 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 155 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 156 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 157 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 158 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 159 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 160 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 161 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 162 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 163 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 164 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 165 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 166 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 167 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 168 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 169 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 170 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 171 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 172 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 173 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 174 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 175 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 176 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 177 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 178 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 179 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 180 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 181 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 182 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 183 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 184 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 185 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 186 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 187 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 188 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 189 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 190 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 191 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 192 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 193 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 194 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 195 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 196 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 197 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 198 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 199 --- Reward: -100 --- Epsilon: 0.009998671593271896\n",
      "Episode: 200 --- Reward: -100 --- Epsilon: 0.009998671593271896\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'C:/Selbststudium/Udemy/Udemy_AI_\\models\\dqn_cartpole.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter10_DeepQNetworks\\cartPoleDqnAgent.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=3'>4</a>\u001b[0m agent\u001b[39m.\u001b[39mtrain(num_episodes\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=4'>5</a>\u001b[0m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPlay?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=5'>6</a>\u001b[0m agent\u001b[39m.\u001b[39;49mplay(num_episodes\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, render\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter10_DeepQNetworks\\cartPoleDqnAgent.ipynb Cell 4'\u001b[0m in \u001b[0;36mAgent.play\u001b[1;34m(self, num_episodes, render)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000002?line=106'>107</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplay\u001b[39m(\u001b[39mself\u001b[39m, num_episodes, render\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000002?line=107'>108</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdqn\u001b[39m.\u001b[39;49mload_model(MODEL_PATH)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000002?line=108'>109</a>\u001b[0m     \u001b[39mfor\u001b[39;00m episode \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_episodes \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000002?line=109'>110</a>\u001b[0m         total_reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter10_DeepQNetworks\\cartPoleDqn.py:48\u001b[0m, in \u001b[0;36mDQN.load_model\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqn.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mself\u001b[39m, path):\n\u001b[1;32m---> <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqn.py?line=47'>48</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minternal_model\u001b[39m.\u001b[39;49mload_weights(path)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m---> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\h5py\\_hl\\files.py:424\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, **kwds)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=421'>422</a>\u001b[0m \u001b[39mwith\u001b[39;00m phil:\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=422'>423</a>\u001b[0m     fapl \u001b[39m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m--> <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=423'>424</a>\u001b[0m     fid \u001b[39m=\u001b[39m make_fid(name, mode, userblock_size,\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=424'>425</a>\u001b[0m                    fapl, fcpl\u001b[39m=\u001b[39mmake_fcpl(track_order\u001b[39m=\u001b[39mtrack_order, fs_strategy\u001b[39m=\u001b[39mfs_strategy,\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=425'>426</a>\u001b[0m                    fs_persist\u001b[39m=\u001b[39mfs_persist, fs_threshold\u001b[39m=\u001b[39mfs_threshold),\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=426'>427</a>\u001b[0m                    swmr\u001b[39m=\u001b[39mswmr)\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=428'>429</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(libver, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=429'>430</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_libver \u001b[39m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\h5py\\_hl\\files.py:190\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=187'>188</a>\u001b[0m     \u001b[39mif\u001b[39;00m swmr \u001b[39mand\u001b[39;00m swmr_support:\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=188'>189</a>\u001b[0m         flags \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=189'>190</a>\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39;49mopen(name, flags, fapl\u001b[39m=\u001b[39;49mfapl)\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=190'>191</a>\u001b[0m \u001b[39melif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/AppData/Roaming/Python/Python39/site-packages/h5py/_hl/files.py?line=191'>192</a>\u001b[0m     fid \u001b[39m=\u001b[39m h5f\u001b[39m.\u001b[39mopen(name, h5f\u001b[39m.\u001b[39mACC_RDWR, fapl\u001b[39m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:96\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'C:/Selbststudium/Udemy/Udemy_AI_\\models\\dqn_cartpole.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter10_DeepQNetworks\\cartPoleDqnAgent.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000004?line=1'>2</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000004?line=2'>3</a>\u001b[0m agent \u001b[39m=\u001b[39m Agent(env)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000004?line=3'>4</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(num_episodes\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000004?line=4'>5</a>\u001b[0m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPlay?\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000004?line=5'>6</a>\u001b[0m agent\u001b[39m.\u001b[39mplay(num_episodes\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, render\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter10_DeepQNetworks\\cartPoleDqnAgent.ipynb Cell 4'\u001b[0m in \u001b[0;36mAgent.train\u001b[1;34m(self, num_episodes)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=59'>60</a>\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m100\u001b[39m \u001b[39m# Verloren \"böse bestrafen\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=60'>61</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremember(state, action, reward, next_state, done)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=61'>62</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=62'>63</a>\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=63'>64</a>\u001b[0m state \u001b[39m=\u001b[39m next_state\n",
      "\u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter10_DeepQNetworks\\cartPoleDqnAgent.ipynb Cell 4'\u001b[0m in \u001b[0;36mAgent.replay\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=88'>89</a>\u001b[0m states_next \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(states_next)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=90'>91</a>\u001b[0m q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdqn(states)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=91'>92</a>\u001b[0m q_values_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_dqn(states_next)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=93'>94</a>\u001b[0m \u001b[39m# Nun folgt die Umsetzung der theoretischen Formel:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqnAgent.ipynb#ch0000003?line=94'>95</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\engine\\base_layer.py:1083\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1078'>1079</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1080'>1081</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1081'>1082</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1082'>1083</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1084'>1085</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1085'>1086</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=89'>90</a>\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=90'>91</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=91'>92</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=92'>93</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=93'>94</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=94'>95</a>\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Selbststudium\\Udemy\\Udemy_AI_\\Chapter10_DeepQNetworks\\cartPoleDqn.py:35\u001b[0m, in \u001b[0;36mDQN.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqn.py?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m     <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqn.py?line=33'>34</a>\u001b[0m     \u001b[39m# Soll model.predict darstellen\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Selbststudium/Udemy/Udemy_AI_/Chapter10_DeepQNetworks/cartPoleDqn.py?line=34'>35</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minternal_model(inputs)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\engine\\base_layer.py:1083\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1078'>1079</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1080'>1081</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1081'>1082</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1082'>1083</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1084'>1085</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1085'>1086</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=89'>90</a>\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=90'>91</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=91'>92</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=92'>93</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=93'>94</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=94'>95</a>\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\engine\\functional.py:451\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=431'>432</a>\u001b[0m \u001b[39m@doc_controls\u001b[39m\u001b[39m.\u001b[39mdo_not_doc_inheritable\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=432'>433</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall\u001b[39m(\u001b[39mself\u001b[39m, inputs, training\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=433'>434</a>\u001b[0m   \u001b[39m\"\"\"Calls the model on new inputs.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=434'>435</a>\u001b[0m \n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=435'>436</a>\u001b[0m \u001b[39m  In this case `call` just reapplies\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=448'>449</a>\u001b[0m \u001b[39m      a list of tensors if there are more than one outputs.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=449'>450</a>\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=450'>451</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_internal_graph(\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=451'>452</a>\u001b[0m       inputs, training\u001b[39m=\u001b[39;49mtraining, mask\u001b[39m=\u001b[39;49mmask)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\engine\\functional.py:589\u001b[0m, in \u001b[0;36mFunctional._run_internal_graph\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=585'>586</a>\u001b[0m   \u001b[39mcontinue\u001b[39;00m  \u001b[39m# Node is not computable, try skipping.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=587'>588</a>\u001b[0m args, kwargs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mmap_arguments(tensor_dict)\n\u001b[1;32m--> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=588'>589</a>\u001b[0m outputs \u001b[39m=\u001b[39m node\u001b[39m.\u001b[39mlayer(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=590'>591</a>\u001b[0m \u001b[39m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/functional.py?line=591'>592</a>\u001b[0m \u001b[39mfor\u001b[39;00m x_id, y \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(node\u001b[39m.\u001b[39mflat_output_ids, tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\engine\\base_layer.py:1083\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1078'>1079</a>\u001b[0m   inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1080'>1081</a>\u001b[0m \u001b[39mwith\u001b[39;00m autocast_variable\u001b[39m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1081'>1082</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1082'>1083</a>\u001b[0m   outputs \u001b[39m=\u001b[39m call_fn(inputs, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1084'>1085</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/engine/base_layer.py?line=1085'>1086</a>\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:92\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=89'>90</a>\u001b[0m bound_signature \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=90'>91</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=91'>92</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=92'>93</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=93'>94</a>\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39m_keras_call_info_injected\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/utils/traceback_utils.py?line=94'>95</a>\u001b[0m     \u001b[39m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\keras\\layers\\core\\dense.py:199\u001b[0m, in \u001b[0;36mDense.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/layers/core/dense.py?line=195'>196</a>\u001b[0m     outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39membedding_lookup_sparse(\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/layers/core/dense.py?line=196'>197</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel, ids, weights, combiner\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/layers/core/dense.py?line=197'>198</a>\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/layers/core/dense.py?line=198'>199</a>\u001b[0m     outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mmatmul(a\u001b[39m=\u001b[39;49minputs, b\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkernel)\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/layers/core/dense.py?line=199'>200</a>\u001b[0m \u001b[39m# Broadcast kernel to inputs.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/layers/core/dense.py?line=200'>201</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/keras/layers/core/dense.py?line=201'>202</a>\u001b[0m   outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtensordot(inputs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel, [[rank \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m], [\u001b[39m0\u001b[39m]])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=147'>148</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=148'>149</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=149'>150</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=150'>151</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/traceback_utils.py?line=151'>152</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1096\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/dispatch.py?line=1093'>1094</a>\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/dispatch.py?line=1094'>1095</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/dispatch.py?line=1095'>1096</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/dispatch.py?line=1096'>1097</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/dispatch.py?line=1097'>1098</a>\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/dispatch.py?line=1098'>1099</a>\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/util/dispatch.py?line=1099'>1100</a>\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3700\u001b[0m, in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/math_ops.py?line=3696'>3697</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39mbatch_mat_mul_v3(\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/math_ops.py?line=3697'>3698</a>\u001b[0m       a, b, adj_x\u001b[39m=\u001b[39madjoint_a, adj_y\u001b[39m=\u001b[39madjoint_b, Tout\u001b[39m=\u001b[39moutput_type, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/math_ops.py?line=3698'>3699</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/math_ops.py?line=3699'>3700</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49mmat_mul(\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/math_ops.py?line=3700'>3701</a>\u001b[0m       a, b, transpose_a\u001b[39m=\u001b[39;49mtranspose_a, transpose_b\u001b[39m=\u001b[39;49mtranspose_b, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_udemy\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6012\u001b[0m, in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6009'>6010</a>\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6010'>6011</a>\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6011'>6012</a>\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6012'>6013</a>\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mMatMul\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, a, b, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_a\u001b[39;49m\u001b[39m\"\u001b[39;49m, transpose_a, \u001b[39m\"\u001b[39;49m\u001b[39mtranspose_b\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6013'>6014</a>\u001b[0m       transpose_b)\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6014'>6015</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   <a href='file:///c%3A/Users/gutsc/anaconda3/envs/tf_udemy/lib/site-packages/tensorflow/python/ops/gen_math_ops.py?line=6015'>6016</a>\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "    agent = Agent(env)\n",
    "    agent.train(num_episodes=200)\n",
    "    input(\"Play?\")\n",
    "    agent.play(num_episodes=10, render=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ad6c57515631d9bb109eb499b4bb1ec6f506277a5ee28599dfb5ae2805d1ee5a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf_udemy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
